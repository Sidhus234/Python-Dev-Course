Wen scrapping usecase: https://www.synerzip.com/blogs/web-scraping-introduction-applications-and-best-practices/

-- Every webpage has two components:
	1. Browser: we use to browse the internet/websites
	2. Server: a computer sitting somewhere in the world which hosts the websites
-- Each webpage has three main files:
	1. HTML: Content of the website (text and images of the website)
	2. CSS: Colors and pretty styling 
	3. JS: (Java script) which allows for different behaviors
	
# To know what websites does and doesn't allow, enter "/robots.txt" after the website
# Ethically, one must limit accessing data to robots.txt allowed list
Some websites also allow api access to download data (API: Application Programming Interface


HOW GOOGLE BOT Works?
Googlebot is bot used by Google to index them
-- Index: create databases of all webpages
-- If robots.txt doesn't mention a webpage, it is allowed for scrapping

